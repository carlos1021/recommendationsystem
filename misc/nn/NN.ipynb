{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c30c3228-f7b1-473b-99e7-958ace9bf93d",
   "metadata": {},
   "source": [
    "## Word2Vec Architecture - Synonym Recommendation\n",
    "\n",
    "\n",
    "#### 1. Load Pretrained Word2Vec Model: Obtain a pre-trained Word2Vec model\n",
    "\n",
    "#### 2. Sentence Preprocessing: Tokenize the sentence\n",
    "\n",
    "#### 3. Contextual Embedding: Calculate the context of the target word by averaging the Word2Vec vectors of the surrounding words.\n",
    "\n",
    "#### 4. Find Synonyms: Use the contextual embedding to find the most similar words to the target word in the model's vocabulary.\n",
    "\n",
    "#### 5. Filter and Rank: Filter out antonyms or irrelevant words and rank the synonyms.\n",
    "\n",
    "#### 6. Return Recommendations: Return the top N recommended synonyms for the target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a876d20-82a9-4fdd-b485-502aef290729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding imports\n",
    "import string\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b19921-1dfb-435c-846c-3e314a1dce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Reading the file and building the dictionary\n",
    "word_vectors = {}\n",
    "with open('glove.6B/glove.6B.100d.txt', 'r') as f:  # Replace with your actual file path\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        word_vectors[word] = vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31246fbc-8083-44d0-af56-c10b9c4bf9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = \"The phrase 'lots of' has many synonyms. For example, some synonyms may include endless, myriad, uncountable, untold, limitless, incalculable, measureless, and more; however, we  should also consider the bounds of this application!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45ab28e4-6bd1-4612-8367-5aa1ddd5e963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The phrase 'lots of' has many synonyms. For example, some synonyms may include endless, myriad, uncountable, untold, limitless, incalculable, measureless, and more; however, we  should also consider the bounds of this application!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e49228-dca6-48a8-bcbc-e64ee14df6e1",
   "metadata": {},
   "source": [
    "#### The Data Science Lifecycle\n",
    "\n",
    "We're going to be involved in the entire process of the pipeline. We are going to have messy data and we need to ensure that it's clean enough to pass into our NN architecture. Running under this assumption will allow us to then analyze the results, and finetune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8c1d878-87d4-49de-bf01-956c3a92d1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the phrase lots of has many synonyms for example some synonyms may include endless myriad uncountable untold limitless incalculable measureless and more however we  should also consider the bounds of this application'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text cleaning\n",
    "cleaned_text = ''.join([word.lower() for word in text_data if word not in string.punctuation])\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd923fc6-9104-4164-8c61-f137d3ff60e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'phrase', 'lots', 'of', 'has', 'many']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize\n",
    "tokens = cleaned_text.split()\n",
    "tokens[0: 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1994b96e-4ec2-4dfa-a5d6-b2b51cc47671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a vocabulary for our tokens\n",
    "vocab_counter = Counter(tokens)\n",
    "vocabulary = {word: id for id, (word, i) in enumerate(vocab_counter.items())}\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebe003d4-0c11-41ef-b108-e62268fee3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocabulary)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b88450e-a599-49d3-be16-be50ea3ae8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c68f61-f4f8-4722-8710-05f1621954b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a30ca412-a6ad-4d48-a4df-edaf385aae3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>[' Sooo sadness I will miss you here in San Di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>['Sooo sadness', 'Sooo saddened', 'Sooo sadden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>['my boss is intimidating me...', 'my boss is ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>['intimidating me', 'harassment me', 'mobbing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>[' what interview! leave me only', ' what inte...</td>\n",
       "      <td>negative</td>\n",
       "      <td>['leave me only', 'leave me solely', 'leave me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>[]</td>\n",
       "      <td>negative</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>['2am feedings for the baby are was pretty fun...</td>\n",
       "      <td>positive</td>\n",
       "      <td>['was pretty funny', 'was funny', 'is funny']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16358</th>\n",
       "      <td>b78ec00df5</td>\n",
       "      <td>['  is appreciated ur night', '  was obtained ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>['is appreciated', 'was obtained', 'be employed']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16359</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>[' wish we could come see u on Denver  husband...</td>\n",
       "      <td>negative</td>\n",
       "      <td>['d closed', 'd collapsed', 'd disclosed']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16360</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>[]</td>\n",
       "      <td>negative</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16361</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>[' Yay recommended for both of you. Enjoy the ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>['Yay recommended for both of you.', 'Yay like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16362</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>[' But it was worthwhile it  ****.', ' But it ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>['But it was worthwhile it  ****.', 'But it wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16363 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "0      549e992a42  [' Sooo sadness I will miss you here in San Di...   \n",
       "1      088c60f138  ['my boss is intimidating me...', 'my boss is ...   \n",
       "2      9642c003ef  [' what interview! leave me only', ' what inte...   \n",
       "3      358bd9e861                                                 []   \n",
       "4      6e0c6d75b1  ['2am feedings for the baby are was pretty fun...   \n",
       "...           ...                                                ...   \n",
       "16358  b78ec00df5  ['  is appreciated ur night', '  was obtained ...   \n",
       "16359  4eac33d1c0  [' wish we could come see u on Denver  husband...   \n",
       "16360  4f4c4fc327                                                 []   \n",
       "16361  f67aae2310  [' Yay recommended for both of you. Enjoy the ...   \n",
       "16362  ed167662a5  [' But it was worthwhile it  ****.', ' But it ...   \n",
       "\n",
       "      sentiment                                      selected_text  \n",
       "0      negative  ['Sooo sadness', 'Sooo saddened', 'Sooo sadden...  \n",
       "1      negative  ['intimidating me', 'harassment me', 'mobbing ...  \n",
       "2      negative  ['leave me only', 'leave me solely', 'leave me...  \n",
       "3      negative                                                 []  \n",
       "4      positive      ['was pretty funny', 'was funny', 'is funny']  \n",
       "...         ...                                                ...  \n",
       "16358  positive  ['is appreciated', 'was obtained', 'be employed']  \n",
       "16359  negative         ['d closed', 'd collapsed', 'd disclosed']  \n",
       "16360  negative                                                 []  \n",
       "16361  positive  ['Yay recommended for both of you.', 'Yay like...  \n",
       "16362  positive  ['But it was worthwhile it  ****.', 'But it wa...  \n",
       "\n",
       "[16363 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"twitter_augmented.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dd032294-0a48-4844-8d17-bbcab6c1fedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 22801\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Text Cleaning and Tokenization\n",
    "def clean_text(text):\n",
    "    # Remove any special characters and numbers\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "# Apply cleaning\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "df['tokenized_text'] = df['cleaned_text'].str.split()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "\n",
    "df['lemmatized_text'] = df['cleaned_text'].apply(lemmatize_text)\n",
    "\n",
    "# Combine all the tokenized text into one list\n",
    "all_sentences = df['tokenized_text'].tolist()\n",
    "\n",
    "\n",
    "# Train the Word2Vec Model\n",
    "word2vec_model = Word2Vec(sentences=all_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "word2vec_model.save(\"word2vec.model\")\n",
    "\n",
    "# Summary\n",
    "print(\"Vocabulary size:\", len(word2vec_model.wv.key_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96ec81eb-a718-4211-b811-65fe5d5511a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>[' Sooo sadness I will miss you here in San Di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>['Sooo sadness', 'Sooo saddened', 'Sooo sadden...</td>\n",
       "      <td>Sooo sadness I will miss you here in San Dieg...</td>\n",
       "      <td>[Sooo, sadness, I, will, miss, you, here, in, ...</td>\n",
       "      <td>[Sooo, sadness, I, will, miss, you, here, in, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>['my boss is intimidating me...', 'my boss is ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>['intimidating me', 'harassment me', 'mobbing ...</td>\n",
       "      <td>my boss is intimidating me my boss is harassme...</td>\n",
       "      <td>[my, boss, is, intimidating, me, my, boss, is,...</td>\n",
       "      <td>[my, bos, is, intimidating, me, my, bos, is, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>[' what interview! leave me only', ' what inte...</td>\n",
       "      <td>negative</td>\n",
       "      <td>['leave me only', 'leave me solely', 'leave me...</td>\n",
       "      <td>what interview leave me only  what interview ...</td>\n",
       "      <td>[what, interview, leave, me, only, what, inter...</td>\n",
       "      <td>[what, interview, leave, me, only, what, inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>[]</td>\n",
       "      <td>negative</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>['2am feedings for the baby are was pretty fun...</td>\n",
       "      <td>positive</td>\n",
       "      <td>['was pretty funny', 'was funny', 'is funny']</td>\n",
       "      <td>am feedings for the baby are was pretty funny ...</td>\n",
       "      <td>[am, feedings, for, the, baby, are, was, prett...</td>\n",
       "      <td>[am, feeding, for, the, baby, are, wa, pretty,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16358</th>\n",
       "      <td>b78ec00df5</td>\n",
       "      <td>['  is appreciated ur night', '  was obtained ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>['is appreciated', 'was obtained', 'be employed']</td>\n",
       "      <td>is appreciated ur night   was obtained ur ni...</td>\n",
       "      <td>[is, appreciated, ur, night, was, obtained, ur...</td>\n",
       "      <td>[is, appreciated, ur, night, wa, obtained, ur,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16359</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>[' wish we could come see u on Denver  husband...</td>\n",
       "      <td>negative</td>\n",
       "      <td>['d closed', 'd collapsed', 'd disclosed']</td>\n",
       "      <td>wish we could come see u on Denver  husband c...</td>\n",
       "      <td>[wish, we, could, come, see, u, on, Denver, hu...</td>\n",
       "      <td>[wish, we, could, come, see, u, on, Denver, hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16360</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>[]</td>\n",
       "      <td>negative</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16361</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>[' Yay recommended for both of you. Enjoy the ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>['Yay recommended for both of you.', 'Yay like...</td>\n",
       "      <td>Yay recommended for both of you Enjoy the bre...</td>\n",
       "      <td>[Yay, recommended, for, both, of, you, Enjoy, ...</td>\n",
       "      <td>[Yay, recommended, for, both, of, you, Enjoy, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16362</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>[' But it was worthwhile it  ****.', ' But it ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>['But it was worthwhile it  ****.', 'But it wa...</td>\n",
       "      <td>But it was worthwhile it    But it was worthy...</td>\n",
       "      <td>[But, it, was, worthwhile, it, But, it, was, w...</td>\n",
       "      <td>[But, it, wa, worthwhile, it, But, it, wa, wor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16363 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "0      549e992a42  [' Sooo sadness I will miss you here in San Di...   \n",
       "1      088c60f138  ['my boss is intimidating me...', 'my boss is ...   \n",
       "2      9642c003ef  [' what interview! leave me only', ' what inte...   \n",
       "3      358bd9e861                                                 []   \n",
       "4      6e0c6d75b1  ['2am feedings for the baby are was pretty fun...   \n",
       "...           ...                                                ...   \n",
       "16358  b78ec00df5  ['  is appreciated ur night', '  was obtained ...   \n",
       "16359  4eac33d1c0  [' wish we could come see u on Denver  husband...   \n",
       "16360  4f4c4fc327                                                 []   \n",
       "16361  f67aae2310  [' Yay recommended for both of you. Enjoy the ...   \n",
       "16362  ed167662a5  [' But it was worthwhile it  ****.', ' But it ...   \n",
       "\n",
       "      sentiment                                      selected_text  \\\n",
       "0      negative  ['Sooo sadness', 'Sooo saddened', 'Sooo sadden...   \n",
       "1      negative  ['intimidating me', 'harassment me', 'mobbing ...   \n",
       "2      negative  ['leave me only', 'leave me solely', 'leave me...   \n",
       "3      negative                                                 []   \n",
       "4      positive      ['was pretty funny', 'was funny', 'is funny']   \n",
       "...         ...                                                ...   \n",
       "16358  positive  ['is appreciated', 'was obtained', 'be employed']   \n",
       "16359  negative         ['d closed', 'd collapsed', 'd disclosed']   \n",
       "16360  negative                                                 []   \n",
       "16361  positive  ['Yay recommended for both of you.', 'Yay like...   \n",
       "16362  positive  ['But it was worthwhile it  ****.', 'But it wa...   \n",
       "\n",
       "                                            cleaned_text  \\\n",
       "0       Sooo sadness I will miss you here in San Dieg...   \n",
       "1      my boss is intimidating me my boss is harassme...   \n",
       "2       what interview leave me only  what interview ...   \n",
       "3                                                          \n",
       "4      am feedings for the baby are was pretty funny ...   \n",
       "...                                                  ...   \n",
       "16358    is appreciated ur night   was obtained ur ni...   \n",
       "16359   wish we could come see u on Denver  husband c...   \n",
       "16360                                                      \n",
       "16361   Yay recommended for both of you Enjoy the bre...   \n",
       "16362   But it was worthwhile it    But it was worthy...   \n",
       "\n",
       "                                          tokenized_text  \\\n",
       "0      [Sooo, sadness, I, will, miss, you, here, in, ...   \n",
       "1      [my, boss, is, intimidating, me, my, boss, is,...   \n",
       "2      [what, interview, leave, me, only, what, inter...   \n",
       "3                                                     []   \n",
       "4      [am, feedings, for, the, baby, are, was, prett...   \n",
       "...                                                  ...   \n",
       "16358  [is, appreciated, ur, night, was, obtained, ur...   \n",
       "16359  [wish, we, could, come, see, u, on, Denver, hu...   \n",
       "16360                                                 []   \n",
       "16361  [Yay, recommended, for, both, of, you, Enjoy, ...   \n",
       "16362  [But, it, was, worthwhile, it, But, it, was, w...   \n",
       "\n",
       "                                         lemmatized_text  \n",
       "0      [Sooo, sadness, I, will, miss, you, here, in, ...  \n",
       "1      [my, bos, is, intimidating, me, my, bos, is, h...  \n",
       "2      [what, interview, leave, me, only, what, inter...  \n",
       "3                                                     []  \n",
       "4      [am, feeding, for, the, baby, are, wa, pretty,...  \n",
       "...                                                  ...  \n",
       "16358  [is, appreciated, ur, night, wa, obtained, ur,...  \n",
       "16359  [wish, we, could, come, see, u, on, Denver, hu...  \n",
       "16360                                                 []  \n",
       "16361  [Yay, recommended, for, both, of, you, Enjoy, ...  \n",
       "16362  [But, it, wa, worthwhile, it, But, it, wa, wor...  \n",
       "\n",
       "[16363 rows x 7 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2fc82e20-508d-4b54-9bca-73044a0b2e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[' what interview! leave me only', ' what interview! leave me solely', ' what interview! leave me single-handedly']\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90a04b59-7fdb-4949-9cc8-1b9d85628319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended synonyms for 'wash' based on sentence context: ['list', 'together', 'realy', 'crossed', 'forums']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def recommend_synonyms(word, sentence, model):\n",
    "    # Tokenize the sentence and remove the target word\n",
    "    sentence_words = sentence.split()\n",
    "    sentence_words = [w for w in sentence_words if w != word]\n",
    "    \n",
    "    # Average the vectors for the words in the sentence\n",
    "    sentence_vec = np.mean([model.wv[w] for w in sentence_words if w in model.wv.key_to_index], axis=0)\n",
    "    \n",
    "    # Find words that are most similar to the averaged sentence vector\n",
    "    similar_words = model.wv.most_similar([sentence_vec], topn=5)\n",
    "    \n",
    "    # Filter out the target word from similar words, if it's present\n",
    "    similar_words = [w for w, sim in similar_words if w != word]\n",
    "    \n",
    "    return similar_words\n",
    "\n",
    "# Test the function\n",
    "sentence = \"Make sure to wash the dishes when you get home.\"\n",
    "word = \"wash\"\n",
    "recommended_synonyms = recommend_synonyms(word, sentence, word2vec_model)\n",
    "print(f\"Recommended synonyms for '{word}' based on sentence context: {recommended_synonyms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57835fe1-3ccc-4b68-808a-f7ff1a7425d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed907e16-71ce-4fbd-84a5-ffeda8f5c7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ca590-f9e9-4a67-b0d5-fd4f161ac488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4fb98d-1336-4d8c-bd07-6805cf67a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Class 9/19/2023'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbacd289-117e-47fd-9fe3-6f49594622e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f9a294-cdb5-4cfe-a67b-1f9dfd863a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3458dac4-c887-47d0-b6e0-56fc1c91f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Have some data to work with\n",
    "\n",
    "Use this data and filter it to our specifications\n",
    "\n",
    "Pass this tokenized filtered text into the model \n",
    "\n",
    "Receive a score (quantitative e.g. 3; qualitative e.g. \"happy\")\n",
    "\n",
    "Run this under all of our data\n",
    "\n",
    "Iterate through our data multiple times (epoch)\n",
    "\n",
    "Continuously evaluate the score and the loss\n",
    "    - loss: the difference between the score received and the actual score\n",
    "    GOAL: reduce loss\n",
    "    we don't want to overfit data (discussion for later)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44f1cd13-512a-4e51-b2a4-8e6ff085f72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecfbca76-80e6-4e5c-a506-65feeae5577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The movie was excellent.\"\n",
    "target_word = \"excellent\"\n",
    "inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    # Extract the hidden states (features) from the BERT model\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# Extract the embeddings for the target word\n",
    "target_word_id = tokenizer.convert_tokens_to_ids(target_word)\n",
    "target_word_index = (inputs['input_ids'][0] == target_word_id).nonzero(as_tuple=True)[0][0]\n",
    "target_word_embedding = hidden_states[0][target_word_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "804fc983-c82b-4851-8783-1ca838008ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"synonyms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81d99123-8d80-4480-b8b5-fc3cf2f26968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lemma             3\n",
       "part_of_speech    0\n",
       "synonyms          2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c4daca9-078b-42ef-84a3-b13db318bc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>synonyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.22-caliber</td>\n",
       "      <td>adjective</td>\n",
       "      <td>.22 caliber;.22 calibre;.22-calibre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.22-calibre</td>\n",
       "      <td>adjective</td>\n",
       "      <td>.22 caliber;.22-caliber;.22 calibre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.22 caliber</td>\n",
       "      <td>adjective</td>\n",
       "      <td>.22-caliber;.22 calibre;.22-calibre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.22 calibre</td>\n",
       "      <td>adjective</td>\n",
       "      <td>.22 caliber;.22-caliber;.22-calibre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.38-caliber</td>\n",
       "      <td>adjective</td>\n",
       "      <td>.38 caliber;.38 calibre;.38-calibre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126996</th>\n",
       "      <td>zero in</td>\n",
       "      <td>verb</td>\n",
       "      <td>range in;home in|zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126997</th>\n",
       "      <td>zip by</td>\n",
       "      <td>verb</td>\n",
       "      <td>fly by;whisk by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126998</th>\n",
       "      <td>zip up</td>\n",
       "      <td>verb</td>\n",
       "      <td>zipper;zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126999</th>\n",
       "      <td>zonk out</td>\n",
       "      <td>verb</td>\n",
       "      <td>pass out;black out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127000</th>\n",
       "      <td>zoom along</td>\n",
       "      <td>verb</td>\n",
       "      <td>zoom;whizz;whizz along</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126996 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              lemma part_of_speech                             synonyms\n",
       "0       .22-caliber      adjective  .22 caliber;.22 calibre;.22-calibre\n",
       "1       .22-calibre      adjective  .22 caliber;.22-caliber;.22 calibre\n",
       "2       .22 caliber      adjective  .22-caliber;.22 calibre;.22-calibre\n",
       "3       .22 calibre      adjective  .22 caliber;.22-caliber;.22-calibre\n",
       "4       .38-caliber      adjective  .38 caliber;.38 calibre;.38-calibre\n",
       "...             ...            ...                                  ...\n",
       "126996      zero in           verb                range in;home in|zero\n",
       "126997       zip by           verb                      fly by;whisk by\n",
       "126998       zip up           verb                           zipper;zip\n",
       "126999     zonk out           verb                   pass out;black out\n",
       "127000   zoom along           verb               zoom;whizz;whizz along\n",
       "\n",
       "[126996 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(axis = 0, inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "227cfc52-b60d-47ff-9b05-104887d65f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lemma             0\n",
       "part_of_speech    0\n",
       "synonyms          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95067dcc-1e49-4490-9bdc-3258660c5050",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edf3b435-61df-4050-8c43-326729b4acf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501796"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pairs #study guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "987678b7-7a85-4097-9fd3-263a52eac399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125450"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54d777bd-33e1-4b23-8754-ee1c309c6f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pairs sample: [('capital of papua new guinea', 'capital of Papua New Guinea', 1), ('myxinikela siroka', 'pivot man', 0), ('napoleon bonaparte', 'Napoleon I', 1), ('silken', 'calvaria', 0), ('lense', 'lens system', 1)]\n",
      "Validation pairs sample: [('bigheaded', 'snotty', 1), ('decampment', 'autumn-flowering', 0), ('overwork', 'richard burbage', 0), ('causing', 'cause', 1), ('firehouse', 'fire station', 1)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# Function to generate synonym and non-synonym pairs\n",
    "def generate_pairs(df):\n",
    "    synonym_pairs = []\n",
    "    non_synonym_pairs = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        lemma = row['lemma']\n",
    "        synonyms = row['synonyms'].split(';')\n",
    "        for synonym in synonyms:\n",
    "            synonym_pairs.append((lemma, synonym, 1))\n",
    "            \n",
    "        # Generate non-synonym pairs by picking random lemmas\n",
    "        non_synonyms = random.choices(df['lemma'].tolist(), k=2)\n",
    "        for non_synonym in non_synonyms:\n",
    "            if non_synonym not in synonyms:\n",
    "                non_synonym_pairs.append((lemma, non_synonym, 0))\n",
    "    \n",
    "    return synonym_pairs, non_synonym_pairs\n",
    "\n",
    "# Generate synonym and non-synonym pairs\n",
    "synonym_pairs, non_synonym_pairs = generate_pairs(df)\n",
    "\n",
    "# Combine the pairs and split into train and validation sets\n",
    "all_pairs = synonym_pairs + non_synonym_pairs\n",
    "random.shuffle(all_pairs)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_pairs, val_pairs = train_test_split(all_pairs, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display some sample pairs\n",
    "print(\"Training pairs sample:\", train_pairs[:5])\n",
    "print(\"Validation pairs sample:\", val_pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90a5ef34-c8b9-41ff-8d0e-73c9a76e276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def prepare_data(pairs):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "    for word1, word2, label in pairs:\n",
    "        encoded_dict = tokenizer(word1, word2, padding='max_length', truncation=True, max_length=50, return_tensors='pt')\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        labels.append(label)\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "train_input_ids, train_attention_masks, train_labels = prepare_data(train_pairs)\n",
    "val_input_ids, val_attention_masks, val_labels = prepare_data(val_pairs)\n",
    "\n",
    "train_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "val_data = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "val_dataloader = DataLoader(val_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "859417da-7c5a-495c-8e0e-2cc904d77069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 17:37:37.701429: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels = 2,  # 0 or 1 for non-synonym or synonym\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf7f76f-0044-488b-972e-50c37eaf8f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save training process for later; run this on a computer with better processing power\n",
    "\n",
    "for epoch in range(1):  # Number of epochs\n",
    "    print(epoch)\n",
    "    print(len(train_dataloader))\n",
    "    for batch in train_dataloader:\n",
    "        print(batch)\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937185cb-6ac3-49e6-84bf-ef7ab95451cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
